{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011447,
     "end_time": "2021-06-04T18:20:08.51834",
     "exception": false,
     "start_time": "2021-06-04T18:20:08.506893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "It's simple. You are given information about the environment of the car crash and you're required to predict the severity of the crash out of 4 level. The predictive system is already built but it needs some data of good quality. You need to prepare the dataset and train the prediction systems.\n",
    "This predictive system will be helpful to anticipate the resources to engage by San Francisco Municipality depending on its severity.\n",
    "### Descriptions\n",
    "You're provided with data about car crashes severity. The file contains 16 features, it represents the car crashes in the city of San Francisco between 2016 and 2020.\n",
    "\n",
    "### File descriptions\n",
    "* <b>train.csv</b> - the training set.\n",
    "* <b>holidays.xml </b>- Information about whether the day is a regular day or a holiday.\n",
    "* <b>weather-sfcsv.csv</b> - Information about the weather.\n",
    "\n",
    "### Data fields\n",
    "* <b>Lat </b>- Latitude of the incident\n",
    "* <b>Lng </b>- Longitude of the incident\n",
    "* <b>Bump, Crossing, Give_Way,Junction, NoExit, RailWay, Roundabout, Stop, Amenity, Side</b> - The characteristics of the location where the incident has taken place, several can be true at the same time. Side is the side of the street.\n",
    "* <b>State</b> - the state from which this dataset is coming from\n",
    "* <b>Distance </b>- the distance of the traffic jam provoked by an accident\n",
    "* <b>Timestamp</b> - the moment when the incident has occurred.\n",
    "* <b>Severity</b> - (Target) An indicator representing the severity of the car crash and possible impacts on the traffic. Values can range from 1 to 4, the highest value translates a highest impact.\n",
    "Severity is the target variable for that exercise. The target variable is the one we want to predict thanks to the predictive system module already present that you will build during that Programming Challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010174,
     "end_time": "2021-06-04T18:20:08.539015",
     "exception": false,
     "start_time": "2021-06-04T18:20:08.528841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import the libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.195537Z",
     "iopub.status.busy": "2021-07-27T15:08:29.195154Z",
     "iopub.status.idle": "2021-07-27T15:08:29.201007Z",
     "shell.execute_reply": "2021-07-27T15:08:29.19989Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.195506Z"
    },
    "papermill": {
     "duration": 0.020965,
     "end_time": "2021-06-04T18:20:08.570429",
     "exception": false,
     "start_time": "2021-06-04T18:20:08.549464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010591,
     "end_time": "2021-06-04T18:20:08.592081",
     "exception": false,
     "start_time": "2021-06-04T18:20:08.58149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "In this step, one should load the data and analyze it. However, I'll load the data and do minimal analysis. You are encouraged to do thorough analysis!\n",
    "\n",
    "Let's load the data using `pandas` and have a look at the generated `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.203126Z",
     "iopub.status.busy": "2021-07-27T15:08:29.202733Z",
     "iopub.status.idle": "2021-07-27T15:08:29.220893Z",
     "shell.execute_reply": "2021-07-27T15:08:29.219772Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.203088Z"
    },
    "papermill": {
     "duration": 0.12872,
     "end_time": "2021-06-04T18:20:08.732163",
     "exception": false,
     "start_time": "2021-06-04T18:20:08.603443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xml_to_df(xml):\n",
    "    root = xml.getroot()\n",
    "    get_range = lambda col: range(len(col))\n",
    "    l = [{r[i].tag:r[i].text for i in get_range(r)} for r in root]\n",
    "    df = pd.DataFrame.from_dict(l)\n",
    "    return df\n",
    "\n",
    "def summarize(df):\n",
    "    print(\"The shape of the dataset is {}.\".format(df.shape), \"\\n\" + 50 * \"=\", \"\\n\", end='')\n",
    "    print(df.head())\n",
    "    print(df.info(), \"\\n\", 50 * \"=\", \"\\n\", end='')\n",
    "    print(df.describe(), \"\\n\", 50 * \"=\", \"\\n\\n\\n\\n\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess & Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.223788Z",
     "iopub.status.busy": "2021-07-27T15:08:29.223165Z",
     "iopub.status.idle": "2021-07-27T15:08:29.246969Z",
     "shell.execute_reply": "2021-07-27T15:08:29.246132Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.223739Z"
    }
   },
   "outputs": [],
   "source": [
    "def reformat_timestamp(main):\n",
    "    acc_date = []\n",
    "    acc_hour = []\n",
    "    main['timestamp'] = pd.to_datetime(main['timestamp'])\n",
    "    for i in main[\"timestamp\"]:\n",
    "        acc_date.append(i.date())\n",
    "        acc_hour.append(i.time().hour)\n",
    "    main[\"acc_date\"] = acc_date\n",
    "    main[\"acc_date\"] = main[\"acc_date\"].astype('datetime64[ns]')\n",
    "    main[\"acc_hour\"] = acc_hour\n",
    "\n",
    "    main.drop(['timestamp'], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "def merge_with_main(main, weather, holidays):\n",
    "    data = pd.merge(main, holidays, how='left', left_on='acc_date', right_on='date')\n",
    "    data = pd.merge(data, weather, how='inner', left_on=['acc_date','acc_hour'], right_on=['w_date','Hour'], left_index=False, right_index=False)\n",
    "    data.reset_index(drop=True)\n",
    "    data = data.drop_duplicates('ID',keep=\"last\")\n",
    "    return data\n",
    "    \n",
    "\n",
    "def fatorize_column(df, column):\n",
    "    codes, uniques = df[column].factorize()\n",
    "    df[column] = codes\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def normalize(df, columns):\n",
    "    result = df.copy()\n",
    "    for column in columns:\n",
    "        max_value = df[column].max()\n",
    "        min_value = df[column].min()\n",
    "        result[column] = df[column] -df[column].mean() / ( max_value-min_value)\n",
    "    return result    \n",
    "    \n",
    "    \n",
    "def preprocess(main, weather, holidays):\n",
    "    # Dropping main useless columns\n",
    "    \n",
    "    main.drop(columns=['Bump',\"Distance(mi)\", 'Give_Way', 'No_Exit', 'Roundabout'], inplace=True)\n",
    "    reformat_timestamp(main)\n",
    "    \n",
    "    # Dropping weather useless columns\n",
    "    intersect = set(weather.columns).intersection(['Selected'])\n",
    "    if(len(intersect)):\n",
    "        weather.drop(columns=['Selected', \"Wind_Chill(F)\",'Humidity(%)', \"Precipitation(in)\"], inplace=True)\n",
    "    weather['w_date'] = pd.to_datetime(weather[['Year', 'Month', 'Day']])\n",
    "\n",
    "    holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "    \n",
    "    # Joinning main data with weather and holidays on Date without duplicates\n",
    "    data = merge_with_main(main, weather, holidays)\n",
    "    \n",
    "    # Replace bool values with (0,1)\n",
    "    #data.replace({False: 0, True: 1}, inplace=True)\n",
    "    \n",
    "    # Replace (R,L) values with (0,1)\n",
    "    data.replace({\"R\": 0, \"L\": 1}, inplace=True)\n",
    "    data['Hour'] = (data['Hour'] % 24 + 4) // 4\n",
    "    data['Hour'].replace({1: 'Late Night',\n",
    "                          2: 'Early Morning',\n",
    "                          3: 'Morning',\n",
    "                          4: 'Noon',\n",
    "                          5: 'Evening',\n",
    "                          6: 'Night'}, inplace=True)\n",
    "    # Factorize Categorical Columns\n",
    "    data = fatorize_column(data, \"Weather_Condition\")\n",
    "    data = fatorize_column(data, \"description\")\n",
    "    data = fatorize_column(data, \"Hour\")\n",
    "    #data = fatorize_column(data, 'day-of-week')\n",
    "\n",
    "    # Dropping merged useless columns\n",
    "    data.drop(columns=['acc_date', 'acc_hour', 'date'], inplace=True)\n",
    "    data['day-of-week'] = data['w_date'].dt.dayofweek\n",
    "    data['w_date'] = data['w_date'].apply(lambda x: x.value) / 10**9\n",
    "\n",
    "\n",
    "\n",
    "    # Impute NAN values with the mean of its column\n",
    "    data.fillna(data.mean(), inplace=True)\n",
    "    \n",
    "    # Dropping rows that have NAN values\n",
    "#     data.dropna(inplace=True)\n",
    "\n",
    "    # Normalizing some numerical columns\n",
    "    #data = normalize(data, [ \"w_date\",'Temperature(F)', 'Wind_Speed(mph)', 'Visibility(mi)'])\n",
    "    A=data[['Temperature(F)',\n",
    "       'Wind_Speed(mph)','Visibility(mi)']]\n",
    "    pca = PCA(n_components=1)            # 2. Instantiate the model with hyperparameters\n",
    "    pca.fit(A)                      # 3. Fit to data. Notice y is not specified!\n",
    "    X_A = pca.transform(A)\n",
    "    data[\"weather1\"]=X_A\n",
    "    data=data.drop(columns=['Temperature(F)',\n",
    "          'Wind_Speed(mph)','Visibility(mi)'])\n",
    "    \n",
    "    \n",
    "#     B=data[['Lat',\n",
    "#        'Lng']]\n",
    "#     pca = PCA(n_components=2)            # 2. Instantiate the model with hyperparameters\n",
    "#     pca.fit(B)                      # 3. Fit to data. Notice y is not specified!\n",
    "#     X_B = pca.transform(B)\n",
    "#     data[\"Lat\"]=X_B[:,0:1]\n",
    "#     data[\"Lng\"]=X_B[:,1:2]\n",
    "    \n",
    "\n",
    "    data=data.drop(columns=['Year', 'Month', 'Hour',\"Day\",\"weather1\",\"Weather_Condition\",\"description\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011003,
     "end_time": "2021-06-04T18:20:08.755399",
     "exception": false,
     "start_time": "2021-06-04T18:20:08.744396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We've got 6407 examples in the dataset with 14 featues, 1 ID, and the `Severity` of the crash.\n",
    "\n",
    "By looking at the features and a sample from the data, the features look of numerical and catogerical types. What about some descriptive statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Clean, Normalize, Impute & Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.248699Z",
     "iopub.status.busy": "2021-07-27T15:08:29.24827Z",
     "iopub.status.idle": "2021-07-27T15:08:29.310219Z",
     "shell.execute_reply": "2021-07-27T15:08:29.30951Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.248668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "dataset_path = '/Datasets/car-crashes-severity-prediction/'\n",
    "#dataset_path = './'\n",
    "df = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\n",
    "\n",
    "main = pd.read_csv(os.path.join(dataset_path, 'train.csv'),index_col=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.311577Z",
     "iopub.status.busy": "2021-07-27T15:08:29.311191Z",
     "iopub.status.idle": "2021-07-27T15:08:29.320682Z",
     "shell.execute_reply": "2021-07-27T15:08:29.319634Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.311547Z"
    }
   },
   "outputs": [],
   "source": [
    "q_low = main[\"Lat\"].quantile(0.25)\n",
    "q_hi = main[\"Lat\"].quantile(0.999)\n",
    "main = main[(main[\"Lat\"] < q_hi) & (main[\"Lat\"] > q_low)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.322145Z",
     "iopub.status.busy": "2021-07-27T15:08:29.321866Z",
     "iopub.status.idle": "2021-07-27T15:08:29.473865Z",
     "shell.execute_reply": "2021-07-27T15:08:29.472534Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.322118Z"
    }
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv(os.path.join(dataset_path, 'weather-sfcsv.csv'),index_col=None)\n",
    "holidays_tree = ET.parse(os.path.join(dataset_path, 'holidays.xml'))\n",
    "holidays = xml_to_df(holidays_tree)\n",
    "\n",
    "# clean & preprocess\n",
    "data = preprocess(main, weather, holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows desciptive statistics for the numerical features, `Lat`, `Lng`, `Distance(mi)`, and `Severity`. I'll use the numerical features to demonstrate how to train the model and make submissions. **However you shouldn't use the numerical features only to make the final submission if you want to make it to the top of the leaderboard.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011808,
     "end_time": "2021-06-04T18:20:08.89742",
     "exception": false,
     "start_time": "2021-06-04T18:20:08.885612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "Now it's time to split the dataset for the training step. Typically the dataset is split into 3 subsets, namely, the training, validation and test sets. In our case, the test set is already predefined. So we'll split the \"training\" set into training and validation sets with 0.8:0.2 ratio. \n",
    "\n",
    "*Note: a good way to generate reproducible results is to set the seed to the algorithms that depends on randomization. This is done with the argument `random_state` in the following command* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.528092Z",
     "iopub.status.busy": "2021-07-27T15:08:29.527561Z",
     "iopub.status.idle": "2021-07-27T15:08:29.542744Z",
     "shell.execute_reply": "2021-07-27T15:08:29.541428Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.528044Z"
    },
    "papermill": {
     "duration": 1.125829,
     "end_time": "2021-06-04T18:20:10.035208",
     "exception": false,
     "start_time": "2021-06-04T18:20:08.909379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=42) # Try adding `stratify` here\n",
    "\n",
    "X_train = train_df.drop(columns=['ID','Severity'])\n",
    "y_train = train_df['Severity']\n",
    "\n",
    "X_val = val_df.drop(columns=['ID','Severity'])\n",
    "y_val = val_df['Severity']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pointed out eariler, I'll use the numerical features to train the classifier. **However, you shouldn't use the numerical features only to make the final submission if you want to make it to the top of the leaderboard.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013313,
     "end_time": "2021-06-04T18:20:10.060544",
     "exception": false,
     "start_time": "2021-06-04T18:20:10.047231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training\n",
    "\n",
    "Let's train a model with the data! We'll train a Random Forest Classifier to demonstrate the process of making submissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.545044Z",
     "iopub.status.busy": "2021-07-27T15:08:29.544491Z",
     "iopub.status.idle": "2021-07-27T15:08:29.829837Z",
     "shell.execute_reply": "2021-07-27T15:08:29.829043Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.544999Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of the classifier\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "# Train the classifier\n",
    "classifier = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022394,
     "end_time": "2021-06-04T18:20:12.327521",
     "exception": false,
     "start_time": "2021-06-04T18:20:12.305127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's test our classifier on the validation dataset and see the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.831666Z",
     "iopub.status.busy": "2021-07-27T15:08:29.831094Z",
     "iopub.status.idle": "2021-07-27T15:08:29.859276Z",
     "shell.execute_reply": "2021-07-27T15:08:29.858247Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.831623Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The accuracy of the classifier on the validation set is \", round(classifier.score(X_val, y_val)*100, 4), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022624,
     "end_time": "2021-06-04T18:20:12.449016",
     "exception": false,
     "start_time": "2021-06-04T18:20:12.426392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Well. That's a good start, right? A classifier that predicts all examples' `Severity` as 2 will get around 0.63. You should get better score as you add more features and do better data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023075,
     "end_time": "2021-06-04T18:20:12.495824",
     "exception": false,
     "start_time": "2021-06-04T18:20:12.472749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission File Generation\n",
    "\n",
    "We have built a model and we'd like to submit our predictions on the test set! In order to do that, we'll load the test set, predict the class and save the submission file. \n",
    "\n",
    "First, we'll load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.86125Z",
     "iopub.status.busy": "2021-07-27T15:08:29.860675Z",
     "iopub.status.idle": "2021-07-27T15:08:29.953888Z",
     "shell.execute_reply": "2021-07-27T15:08:29.952834Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.861206Z"
    },
    "papermill": {
     "duration": 0.056131,
     "end_time": "2021-06-04T18:20:12.568677",
     "exception": false,
     "start_time": "2021-06-04T18:20:12.512546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(dataset_path, 'test.csv'))\n",
    "test_df = preprocess(test_df, weather, holidays)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T15:08:29.955849Z",
     "iopub.status.busy": "2021-07-27T15:08:29.955441Z",
     "iopub.status.idle": "2021-07-27T15:08:30.001979Z",
     "shell.execute_reply": "2021-07-27T15:08:30.001018Z",
     "shell.execute_reply.started": "2021-07-27T15:08:29.955804Z"
    },
    "papermill": {
     "duration": 0.057034,
     "end_time": "2021-06-04T18:20:12.663409",
     "exception": false,
     "start_time": "2021-06-04T18:20:12.606375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = test_df.drop(columns=['ID'])\n",
    "\n",
    "# You should update/remove the next line once you change the features used for training\n",
    "# X_test = X_test[['Lat', 'Lng', 'Distance(mi)']]\n",
    "\n",
    "y_test_predicted = classifier.predict(X_test)\n",
    "\n",
    "test_df['Severity'] = y_test_predicted\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025364,
     "end_time": "2021-06-04T18:20:12.713157",
     "exception": false,
     "start_time": "2021-06-04T18:20:12.687793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we're ready to generate the submission file. The submission file needs the columns `ID` and `Severity` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
